{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2890544c-b35b-450c-9eb1-9f423d99c65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('AdaBins')\n",
    "sys.path.append('CLIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9dfa4f3-c849-427f-abcf-c5905ba80f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olga/depth_prediction/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from munch import Munch\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision.transforms import ToTensor, Normalize, Resize, Compose\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from datasets import *\n",
    "from model import Model\n",
    "from transforms import *\n",
    "from utils import compute_metrics\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "os.makedirs('checkpoint', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3e4c7a0-bbc4-4037-84da-eb22dfd8692d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dbccce8-bd96-4252-af76-ba298379b7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(min_val=1e-3, max_val=180, K=100, norm='linear')\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load('checkpoint/model_73930.ckpt'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba024983-ac22-4a74-8503-89765f8549e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3a3e716d-aa7e-47f2-873e-ca4aa58ebe0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 118287/118287 [4:58:32<00:00,  6.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Munch({'rel': 1.1433313165791876, 'rms': 6.757456747567729, 'log_10': 0.17277100761165964, 'sq_rel': 5.192733242787379, 'rmse_log': 0.5352806439334903, 'threshold_acc_1': 0.34421077874153405, 'threshold_acc_2': 0.6807899167752951, 'threshold_acc_3': 0.8751707355257307})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMG_PATH = 'train2017'\n",
    "DEPTH_PATH = 'COCO_midas_depth'\n",
    "\n",
    "img_transform = Compose([\n",
    "    ToTensor(),\n",
    "    Resize((224,224), interpolation=Image.BILINEAR),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_metrics = Munch(\n",
    "    rel=0, rms=0, log_10=0, sq_rel=0, rmse_log=0,\n",
    "    threshold_acc_1=0, threshold_acc_2=0, threshold_acc_3=0)\n",
    "\n",
    "cnt = 0\n",
    "for image_name in tqdm(os.listdir(IMG_PATH)):\n",
    "    cnt += 1\n",
    "    image_path = os.path.join(IMG_PATH, image_name)\n",
    "    depth_path = os.path.join(DEPTH_PATH, image_name.replace('.jpg', '.npy'))\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = img_transform(image).unsqueeze(0)\n",
    "\n",
    "    depth_gt = np.load(depth_path)[np.newaxis, ...]\n",
    "    with torch.no_grad():\n",
    "        pred = model(image.to(device))[-1].cpu()\n",
    "        pred = torch.clip(pred, 1e-3, 180)\n",
    "        image = torch.flip(image, [-1])\n",
    "        pred_lr = model(image.to(device))[-1].cpu()\n",
    "        pred_lr = torch.flip(pred_lr, [-1])\n",
    "        pred_lr = torch.clip(pred_lr, 1e-3, 180)\n",
    "        \n",
    "    final = 0.5 * (pred + pred_lr)\n",
    "    if final.shape != depth_gt.shape:\n",
    "        final = nn.functional.interpolate(final, depth_gt.shape[-2:], mode='bilinear', align_corners=True)[0].numpy()\n",
    "        \n",
    "    final[final < 1e-3] = 1e-3\n",
    "    final[final > 180] = 180\n",
    "    final[np.isinf(final)] = 180\n",
    "    final[np.isnan(final)] = 1e-3\n",
    "    \n",
    "    mask = (depth_gt > 1e-3) * (depth_gt < 180)\n",
    "    metrics = compute_metrics(depth_gt[mask], final[mask])\n",
    "    for m in metrics.keys():\n",
    "        test_metrics[m] += metrics[m]\n",
    "\n",
    "for m in metrics.keys():\n",
    "    test_metrics[m] /= cnt\n",
    "\n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf2aa31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
